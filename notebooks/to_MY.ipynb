{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ikboljonsobirov/hecktor/fusion_vit/fusion_vit_project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikboljonsobirov/.conda/envs/qkv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss, DiceCELoss, FocalLoss, GeneralizedDiceFocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    Compose,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR, UNet, SegResNet, UNETR\n",
    "from monai import data\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "from functools import partial\n",
    "from src.data.augmentations import *\n",
    "\n",
    "import torch\n",
    "import SimpleITK as sitk\n",
    "from src.models.components.metrics import dice\n",
    "from src.models.components.unetr.unetr import CustomUNETR\n",
    "\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = np.where(self.count > 0, self.sum / self.count, self.sum)\n",
    "\n",
    "\n",
    "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k in d:\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "\n",
    "    return tr, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = (96,96,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size, sw_batch_size, data_dir, json_list, fold):\n",
    "    data_dir = data_dir\n",
    "    datalist_json = json_list\n",
    "\n",
    "    train_files, validation_files = datafold_read(datalist=datalist_json, basedir=data_dir, fold=fold)\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\",\"image2\", \"label\"], ensure_channel_first = True),\n",
    "            transforms.SpatialPadd(keys=[\"image\", \"label\"], spatial_size=(roi[0], roi[1], roi[2]), method='end'),\n",
    "            transforms.RandCropByPosNegLabeld(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                label_key=\"label\",\n",
    "                spatial_size=roi,\n",
    "                pos=1,\n",
    "                neg=1,\n",
    "                num_samples=1,\n",
    "                image_key=\"image\",\n",
    "                image_threshold=0,\n",
    "            ),\n",
    "            transforms.NormalizeIntensityd(keys=[\"image\", \"image2\"]),\n",
    " \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\",\"image2\", \"label\"], ensure_channel_first = True),\n",
    "            transforms.SpatialPadd(keys=[\"image\", \"label\"], spatial_size=(roi[0], roi[1], roi[2]), method='end'),\n",
    "        ]            \n",
    "    )\n",
    "    train_ds = data.Dataset(data=train_files, transform=train_transform)\n",
    "\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_ds = data.Dataset(data=validation_files, transform=val_transform)\n",
    "\n",
    "    val_loader = data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"\"\n",
    "\n",
    "data_dir = '/share/nvmedata/ikboljonsobirov/fusion_vit/hecktor2022_cropped/'\n",
    "datalist_json = '/home/ikboljonsobirov/hecktor/fusion_vit/fusion_vit_project/files/train_json_new.json'\n",
    "batch_size = 2\n",
    "sw_batch_size = 8\n",
    "fold = 0 # 0,1,2,3,4\n",
    "infer_overlap = 0.5\n",
    "\n",
    "chkpt_path = '/home/ikboljonsobirov/hecktor/fusion_vit/fusion_vit_project/logs/train/runs/2023-10-27_12-13-58/checkpoints/epoch_076.ckpt'\n",
    "# chkpt_path = '/home/ikboljonsobirov/hecktor/fusion_vit/fusion_vit_project/logs/train/runs/segresnet_fold0/checkpoints/epoch_088.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, train_ds, val_ds = get_loader(batch_size, sw_batch_size, data_dir, datalist_json, fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qkv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
